{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f285431c",
   "metadata": {},
   "source": [
    "# 사진_유사도_비교_서비스_mediapipe_안면인식_인증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60215705",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O embedder.tflite -q https://storage.googleapis.com/mediapipe-models/image_embedder/mobilenet_v3_small/float32/1/mobilenet_v3_small.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd58a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMAGE_FILENAMES = ['burger.jpg', 'burger_crop.jpg']\n",
    "\n",
    "for name in IMAGE_FILENAMES:\n",
    "    url = f'https://storage.googleapis.com/mediapipe-assets/{name}'\n",
    "    urllib.request.urlretrieve(url, name)\n",
    "\n",
    "DESIRED_HEIGHT = 480\n",
    "DESIRED_WIDTH = 480\n",
    "\n",
    "def resize_and_show(image):\n",
    "    h, w = image.shape[:2]\n",
    "    if h < w:\n",
    "        img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
    "    else:\n",
    "        img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
    "    return img\n",
    "\n",
    "# 이미지 미리보기\n",
    "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
    "for name, image in images.items():\n",
    "    print(name)\n",
    "    resized_image = resize_and_show(image)\n",
    "    plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(name)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# Create options for Image Embedder\n",
    "base_options = python.BaseOptions(model_asset_path='embedder.tflite')\n",
    "l2_normalize = True #@param {type:\"boolean\"}\n",
    "quantize = True #@param {type:\"boolean\"}\n",
    "options = vision.ImageEmbedderOptions(\n",
    "    base_options=base_options, l2_normalize=l2_normalize, quantize=quantize)\n",
    "\n",
    "\n",
    "# Create Image Embedder\n",
    "with vision.ImageEmbedder.create_from_options(options) as embedder:\n",
    "\n",
    "    # Format images for MediaPipe\n",
    "    first_image = mp.Image.create_from_file(IMAGE_FILENAMES[0])\n",
    "    second_image = mp.Image.create_from_file(IMAGE_FILENAMES[1])\n",
    "    first_embedding_result = embedder.embed(first_image)\n",
    "    second_embedding_result = embedder.embed(second_image)\n",
    "\n",
    "    # Calculate and print similarity\n",
    "    similarity = vision.ImageEmbedder.cosine_similarity(\n",
    "      first_embedding_result.embeddings[0],\n",
    "      second_embedding_result.embeddings[0])\n",
    "    print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac180be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ada40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import urllib\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# 이미지 임베딩 및 유사도 계산 함수\n",
    "def image_to_text(image1, image2):\n",
    "    # 이미지 파일 저장\n",
    "    cv2.imwrite(\"image1.jpg\", image1)\n",
    "    cv2.imwrite(\"image2.jpg\", image2)\n",
    "    \n",
    "    # Create options for Image Embedder\n",
    "    base_options = python.BaseOptions(model_asset_path='./mobilenet_v3_large.tflite')\n",
    "    l2_normalize = True\n",
    "    quantize = True\n",
    "    options = vision.ImageEmbedderOptions(\n",
    "        base_options=base_options, l2_normalize=l2_normalize, quantize=quantize)\n",
    "\n",
    "    # Create Image Embedder\n",
    "    with vision.ImageEmbedder.create_from_options(options) as embedder:\n",
    "        # Format images for MediaPipe\n",
    "        first_image = mp.Image.create_from_file(\"image1.jpg\")\n",
    "        second_image = mp.Image.create_from_file(\"image2.jpg\")\n",
    "        first_embedding_result = embedder.embed(first_image)\n",
    "        second_embedding_result = embedder.embed(second_image)\n",
    "\n",
    "        # Calculate and print similarity\n",
    "        similarity = vision.ImageEmbedder.cosine_similarity(\n",
    "            first_embedding_result.embeddings[0],\n",
    "            second_embedding_result.embeddings[0])\n",
    "        return f\"Similarity: {similarity:.4f}\"\n",
    "\n",
    "# Gradio 인터페이스 생성\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# Image Similarity Comparison\")\n",
    "    \n",
    "    with gr.Tab(\"Image Upload\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                image1 = gr.Image(label=\"First Image\")\n",
    "            with gr.Column():\n",
    "                image2 = gr.Image(label=\"Second Image\")\n",
    "        output = gr.Textbox(label=\"Similarity Output\")\n",
    "        convert_btn = gr.Button(\"Compare\")\n",
    "        convert_btn.click(\n",
    "            fn=image_to_text, inputs=[image1, image2], outputs=output\n",
    "        )\n",
    "\n",
    "app.launch(inline=False, share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d9a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21d6158b",
   "metadata": {},
   "source": [
    "# Deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03176475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257722d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1451e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "# 얼굴 임베딩 및 유사도 계산 함수\n",
    "def image_to_text(image1, image2):\n",
    "    # 이미지 파일 저장\n",
    "    cv2.imwrite(\"image1.jpg\", image1)\n",
    "    cv2.imwrite(\"image2.jpg\", image2)\n",
    "    \n",
    "    # VGG-Face 모델을 사용하여 얼굴 유사도 계산\n",
    "    result = DeepFace.verify(\"image1.jpg\", \"image2.jpg\", model_name='VGG-Face')\n",
    "    \n",
    "    # 거리 값을 기준으로 유사도 출력\n",
    "    similarity = result['distance']\n",
    "    return f\"Similarity: {similarity:.4f}\"\n",
    "\n",
    "# Gradio 인터페이스 생성\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# Face Similarity Comparison using DeepFace\")\n",
    "    \n",
    "    with gr.Tab(\"Image Upload\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                image1 = gr.Image(label=\"First Image\")\n",
    "            with gr.Column():\n",
    "                image2 = gr.Image(label=\"Second Image\")\n",
    "        output = gr.Textbox(label=\"Similarity Output\")\n",
    "        convert_btn = gr.Button(\"Compare\")\n",
    "        convert_btn.click(\n",
    "            fn=image_to_text, inputs=[image1, image2], outputs=output\n",
    "        )\n",
    "\n",
    "app.launch(inline=False, share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e3aee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "models = [\n",
    "  \"VGG-Face\", \n",
    "  \"Facenet\", \n",
    "  \"Facenet512\", \n",
    "  \"OpenFace\", \n",
    "  \"DeepFace\", \n",
    "  \"DeepID\", \n",
    "  \"ArcFace\", \n",
    "  \"Dlib\", \n",
    "  \"SFace\",\n",
    "  \"GhostFaceNet\",\n",
    "]\n",
    "\n",
    "metrics = [\"cosine\", \"euclidean\", \"euclidean_l2\"]\n",
    "\n",
    "# 얼굴 임베딩 및 유사도 계산 함수\n",
    "def image_to_text(image1, image2, model, metric):\n",
    "    # 이미지 파일 저장\n",
    "    cv2.imwrite(\"image1.jpg\", image1)\n",
    "    cv2.imwrite(\"image2.jpg\", image2)\n",
    "    \n",
    "    # 얼굴 임베딩 생성\n",
    "    embedding_objs1 = DeepFace.represent(img_path=\"image1.jpg\", model_name=model)\n",
    "    embedding_objs2 = DeepFace.represent(img_path=\"image2.jpg\", model_name=model)\n",
    "    \n",
    "    # 임베딩 벡터 추출\n",
    "    embedding1 = embedding_objs1[0][\"embedding\"]\n",
    "    embedding2 = embedding_objs2[0][\"embedding\"]\n",
    "    \n",
    "    # 유사도 계산\n",
    "    result = DeepFace.verify(\n",
    "        img1_path=\"image1.jpg\", \n",
    "        img2_path=\"image2.jpg\", \n",
    "        model_name=model,\n",
    "        distance_metric=metric\n",
    "    )\n",
    "    similarity = result['distance']\n",
    "    return f\"Similarity: {similarity:.4f}\"\n",
    "\n",
    "# Gradio 인터페이스 생성\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# Face Similarity Comparison using DeepFace\")\n",
    "    \n",
    "    with gr.Tab(\"Image Upload\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                image1 = gr.Image(label=\"First Image\")\n",
    "            with gr.Column():\n",
    "                image2 = gr.Image(label=\"Second Image\")\n",
    "        model = gr.Dropdown(label=\"Model\", choices=models, value=\"VGG-Face\")\n",
    "        metric = gr.Dropdown(label=\"Distance Metric\", choices=metrics, value=\"cosine\")\n",
    "        output = gr.Textbox(label=\"Similarity Output\")\n",
    "        convert_btn = gr.Button(\"Compare\")\n",
    "        convert_btn.click(\n",
    "            fn=image_to_text, inputs=[image1, image2, model, metric], outputs=output\n",
    "        )\n",
    "\n",
    "app.launch(inline=False, share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cfcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f6eecf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/gradio_client/documentation.py:106: UserWarning: Could not get documentation group for <class 'gradio.mix.Parallel'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
      "/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/gradio_client/documentation.py:106: UserWarning: Could not get documentation group for <class 'gradio.mix.Series'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
      "2024-09-24 15:03:56.689343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-24 15:03:56.714035: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-24 15:03:56.720826: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-24 15:03:56.736429: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 15:03:58.022041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'build_model' from partially initialized module 'deepface.modules.modeling' (most likely due to a circular import) (/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/deepface/modules/modeling.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepFace\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.11/site-packages/deepface/DeepFace.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m package_utils, folder_utils\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logger\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     modeling,\n\u001b[1;32m     22\u001b[0m     representation,\n\u001b[1;32m     23\u001b[0m     verification,\n\u001b[1;32m     24\u001b[0m     recognition,\n\u001b[1;32m     25\u001b[0m     demography,\n\u001b[1;32m     26\u001b[0m     detection,\n\u001b[1;32m     27\u001b[0m     streaming,\n\u001b[1;32m     28\u001b[0m     preprocessing,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     32\u001b[0m logger \u001b[38;5;241m=\u001b[39m Logger()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.11/site-packages/deepface/modules/modeling.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# project dependencies\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfacial_recognition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     VGGFace,\n\u001b[1;32m      7\u001b[0m     OpenFace,\n\u001b[1;32m      8\u001b[0m     FbDeepFace,\n\u001b[1;32m      9\u001b[0m     DeepID,\n\u001b[1;32m     10\u001b[0m     ArcFace,\n\u001b[1;32m     11\u001b[0m     SFace,\n\u001b[1;32m     12\u001b[0m     Dlib,\n\u001b[1;32m     13\u001b[0m     Facenet,\n\u001b[1;32m     14\u001b[0m     GhostFaceNet,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     FastMtCnn,\n\u001b[1;32m     18\u001b[0m     MediaPipe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     CenterFace,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdemography\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Age, Gender, Race, Emotion\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.11/site-packages/deepface/models/facial_recognition/VGGFace.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m package_utils, folder_utils\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m verification\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFacialRecognition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FacialRecognition\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logger\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.11/site-packages/deepface/modules/verification.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# project dependencies\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m representation, detection, modeling\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFacialRecognition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FacialRecognition\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logger\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.11/site-packages/deepface/modules/representation.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# project dependencies\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_model\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_utils\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m modeling, detection, preprocessing\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'build_model' from partially initialized module 'deepface.modules.modeling' (most likely due to a circular import) (/home/user/miniforge3/envs/torch/lib/python3.11/site-packages/deepface/modules/modeling.py)"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# 얼굴 임베딩 및 유사도 계산 함수\n",
    "def image_to_text(image1, image2):\n",
    "    # 이미지 파일 저장\n",
    "    cv2.imwrite(\"image1.jpg\", image1)\n",
    "    cv2.imwrite(\"image2.jpg\", image2)\n",
    "    \n",
    "    # VGG-Face 모델을 사용하여 임베딩 생성\n",
    "    embedding_objs1 = DeepFace.represent(img_path=\"image1.jpg\", model_name=\"VGG-Face\")\n",
    "    embedding_objs2 = DeepFace.represent(img_path=\"image2.jpg\", model_name=\"VGG-Face\")\n",
    "    \n",
    "    # 임베딩 추출\n",
    "    embedding1 = embedding_objs1[0][\"embedding\"]\n",
    "    embedding2 = embedding_objs2[0][\"embedding\"]\n",
    "    \n",
    "    # 코사인 유사도를 계산\n",
    "    cosine_similarity = 1 - distance.cosine(embedding1, embedding2)\n",
    "    \n",
    "    return f\"Cosine Similarity: {cosine_similarity:.4f}\"\n",
    "\n",
    "# Gradio 인터페이스 생성\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# Face Similarity Comparison using DeepFace (Embeddings)\")\n",
    "    \n",
    "    with gr.Tab(\"Image Upload\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                image1 = gr.Image(label=\"First Image\")\n",
    "            with gr.Column():\n",
    "                image2 = gr.Image(label=\"Second Image\")\n",
    "        output = gr.Textbox(label=\"Similarity Output\")\n",
    "        convert_btn = gr.Button(\"Compare\")\n",
    "        convert_btn.click(\n",
    "            fn=image_to_text, inputs=[image1, image2], outputs=output\n",
    "        )\n",
    "\n",
    "app.launch(inline=False, share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d2322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9433d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca56ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ba6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
